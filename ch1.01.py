import aiohttp
import asyncio
from bs4 import BeautifulSoup
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes
import urllib.parse
import random


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üîç *–ë–æ—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏*\n\n"
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—ã:\n"
        "‚Ä¢ /search <–∑–∞–ø—Ä–æ—Å> - –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n"
        "‚Ä¢ /news - –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏\n"
        "‚Ä¢ /weather - –ø–æ–≥–æ–¥–∞ (–ú–æ—Å–∫–≤–∞)\n"
        "‚Ä¢ /help - –ø–æ–º–æ—â—å\n\n"
        "–ü—Ä–∏–º–µ—Ä: `/search —Ä–µ—Ü–µ–ø—Ç –ø–∏—Ü—Ü—ã`",
        parse_mode='Markdown'
    )


async def search_google_scraper(query: str) -> str:
    """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google (–≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥)"""
    try:
        # –°–æ–∑–¥–∞–µ–º URL –¥–ª—è –ø–æ–∏—Å–∫–∞
        search_url = f"https://www.google.com/search?q={urllib.parse.quote(query)}&hl=ru"

        # –°–ª—É—á–∞–π–Ω—ã–µ User-Agent –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
            'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15'
        ]

        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept-Language': 'ru-RU,ru;q=0.9',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

        async with aiohttp.ClientSession() as session:
            async with session.get(search_url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    results = []

                    # –ü–∞—Ä—Å–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    for g in soup.find_all('div', class_='g'):
                        # –ù–∞–∑–≤–∞–Ω–∏–µ
                        title_element = g.find('h3')
                        if title_element:
                            title = title_element.get_text()
                        else:
                            continue

                        # –û–ø–∏—Å–∞–Ω–∏–µ
                        desc_element = g.find('div', class_='VwiC3b') or g.find('div', class_='lyLwlc')
                        description = desc_element.get_text() if desc_element else "–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è"

                        # –°—Å—ã–ª–∫–∞
                        link_element = g.find('a')
                        link = link_element['href'] if link_element and 'href' in link_element.attrs else ""

                        if title and link:
                            results.append({
                                'title': title,
                                'description': description[:300],
                                'link': link
                            })

                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –æ–±—ã—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∏—â–µ–º –±—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã
                    if not results:
                        # –ë—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç (featured snippet)
                        quick_answer = soup.find('div', class_='BNeawe')
                        if quick_answer:
                            return f"üìã *–ë—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç:*\n\n{quick_answer.get_text()[:2000]}"

                        # –ó–Ω–∞–Ω–∏—è (knowledge graph)
                        knowledge = soup.find('div', class_='kno-rdesc')
                        if knowledge:
                            return f"üìö *–ó–Ω–∞–Ω–∏—è:*\n\n{knowledge.get_text()[:2000]}"

                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    if results:
                        response_text = f"üîç *–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É:* `{query}`\n\n"
                        for i, result in enumerate(results[:5], 1):
                            response_text += f"*{i}. {result['title']}*\n"
                            response_text += f"{result['description']}\n"
                            if result['link']:
                                # –û—á–∏—â–∞–µ–º —Å—Å—ã–ª–∫—É –æ—Ç –º—É—Å–æ—Ä–∞
                                clean_link = result['link'].split('&')[0].replace('/url?q=', '')
                                if clean_link.startswith('http'):
                                    response_text += f"üîó {clean_link}\n"
                            response_text += "\n"

                        if len(response_text) > 4000:
                            response_text = response_text[:4000] + "...\n\n‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–µ–∑–∞–Ω—ã"

                        return response_text

                    return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å."

                return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {response.status}"

    except asyncio.TimeoutError:
        return "‚è∞ –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    except Exception as e:
        return f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {str(e)}"


async def search(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û—Å–Ω–æ–≤–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ –ø–æ–∏—Å–∫–∞"""
    if not context.args:
        await update.message.reply_text(
            "üìù *–§–æ—Ä–º–∞—Ç:* `/search –≤–∞—à –∑–∞–ø—Ä–æ—Å`\n\n"
            "üìå *–ü—Ä–∏–º–µ—Ä—ã:*\n"
            "‚Ä¢ `/search –ø–æ–≥–æ–¥–∞ –ú–æ—Å–∫–≤–∞`\n"
            "‚Ä¢ `/search —Ä–µ—Ü–µ–ø—Ç –±–æ—Ä—â–∞`\n"
            "‚Ä¢ `/search –Ω–æ–≤–æ—Å—Ç–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π`",
            parse_mode='Markdown'
        )
        return

    query = ' '.join(context.args)

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –Ω–∞—á–∞–ª–µ –ø–æ–∏—Å–∫–∞
    search_msg = await update.message.reply_text(f"üîç *–ò—â—É:* `{query}`\n\n‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞...", parse_mode='Markdown')

    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
    result = await search_google_scraper(query)

    # –†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
    await search_msg.edit_text(result, parse_mode='Markdown')


async def news(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"""
    try:
        await update.message.reply_text("üì∞ *–ò—â—É —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏...*", parse_mode='Markdown')

        async with aiohttp.ClientSession() as session:
            # –ù–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ RSS
            urls = [
                "https://news.google.com/rss?hl=ru&gl=RU&ceid=RU:ru",
                "https://lenta.ru/rss/news",
                "https://ria.ru/export/rss2/index.xml"
            ]

            news_items = []

            for url in urls:
                try:
                    async with session.get(url, timeout=5) as response:
                        if response.status == 200:
                            text = await response.text()
                            soup = BeautifulSoup(text, 'xml')

                            items = soup.find_all('item')[:3]  # –ë–µ—Ä–µ–º –ø–æ 3 –Ω–æ–≤–æ—Å—Ç–∏ —Å –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                            for item in items:
                                title = item.title.get_text() if item.title else ""
                                link = item.link.get_text() if item.link else ""
                                pub_date = item.pubDate.get_text() if item.pubDate else ""

                                if title and link:
                                    news_items.append({
                                        'title': title[:200],
                                        'link': link,
                                        'source': url.split('/')[2]
                                    })
                except:
                    continue

            if news_items:
                response_text = "üì∞ *–ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:*\n\n"
                for i, news in enumerate(news_items[:10], 1):
                    response_text += f"*{i}. {news['title']}*\n"
                    response_text += f"üîó {news['link']}\n"
                    response_text += f"üì° –ò—Å—Ç–æ—á–Ω–∏–∫: {news['source']}\n\n"

                await update.message.reply_text(response_text, parse_mode='Markdown')
            else:
                await update.message.reply_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏")

    except Exception as e:
        await update.message.reply_text(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {str(e)}")


async def weather(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ü–æ–≥–æ–¥–∞"""
    try:
        city = ' '.join(context.args) if context.args else "–ú–æ—Å–∫–≤–∞"

        await update.message.reply_text(f"üå§ *–ó–∞–ø—Ä–∞—à–∏–≤–∞—é –ø–æ–≥–æ–¥—É –¥–ª—è:* {city}", parse_mode='Markdown')

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ø–Ω–¥–µ–∫—Å.–ü–æ–≥–æ–¥–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
        async with aiohttp.ClientSession() as session:
            # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            search_query = f"–ø–æ–≥–æ–¥–∞ {city} —Å–µ–≥–æ–¥–Ω—è"
            search_url = f"https://www.google.com/search?q={urllib.parse.quote(search_query)}&hl=ru"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            async with session.get(search_url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ
                    weather_info = soup.find('div', class_='BNeawe')

                    if weather_info:
                        temp = weather_info.get_text()
                        await update.message.reply_text(
                            f"üå§ *–ü–æ–≥–æ–¥–∞ –≤ {city}:*\n\n"
                            f"{temp}\n\n"
                            f"‚ÑπÔ∏è –î–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–≥–æ–¥–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è",
                            parse_mode='Markdown'
                        )
                    else:
                        await update.message.reply_text(
                            f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–≥–æ–¥—É –¥–ª—è {city}\n\n"
                            f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ: `/weather –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥`",
                            parse_mode='Markdown'
                        )

    except Exception as e:
        await update.message.reply_text(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {str(e)}")


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–°–ø—Ä–∞–≤–∫–∞"""
    help_text = """
üîç *–ü–æ–∏—Å–∫–æ–≤—ã–π –±–æ—Ç*

*–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:*

üîé *–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:*
`/search <–∑–∞–ø—Ä–æ—Å>` - –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
–ü—Ä–∏–º–µ—Ä—ã:
‚Ä¢ `/search —Ä–µ—Ü–µ–ø—Ç –±–ª–∏–Ω–æ–≤`
‚Ä¢ `/search –Ω–æ–≤–æ—Å—Ç–∏ —Ñ—É—Ç–±–æ–ª–∞`
‚Ä¢ `/search —á—Ç–æ —Ç–∞–∫–æ–µ –ò–ò`
‚Ä¢ `/search —Ñ–∏–ª—å–º—ã 2024`

üì∞ *–ù–æ–≤–æ—Å—Ç–∏:*
`/news` - –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –†–æ—Å—Å–∏–∏ –∏ –º–∏—Ä–∞

üå§ *–ü–æ–≥–æ–¥–∞:*
`/weather [–≥–æ—Ä–æ–¥]` - –ø–æ–≥–æ–¥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ú–æ—Å–∫–≤–∞)
–ü—Ä–∏–º–µ—Ä—ã:
‚Ä¢ `/weather`
‚Ä¢ `/weather –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥`
‚Ä¢ `/weather –õ–æ–Ω–¥–æ–Ω`

‚ÑπÔ∏è *–î—Ä—É–≥–æ–µ:*
`/start` - –Ω–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã
`/help` - —ç—Ç–∞ —Å–ø—Ä–∞–≤–∫–∞

*–°–æ–≤–µ—Ç—ã –ø–æ –ø–æ–∏—Å–∫—É:*
‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
‚Ä¢ –ù–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ —á–∞—Å—Ç–æ –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
‚Ä¢ –î–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–∞–≤—ã—á–∫–∏: `/search "–ò–ª–æ–Ω –ú–∞—Å–∫"`

üì± *–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:*
‚Ä¢ –ú–∞–∫—Å–∏–º—É–º 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å
‚Ä¢ –ò–Ω–æ–≥–¥–∞ –ø–æ–∏—Å–∫ –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å
‚Ä¢ –ù–µ—Ç 100% –≥–∞—Ä–∞–Ω—Ç–∏–∏ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    """
    await update.message.reply_text(help_text, parse_mode='Markdown')


async def echo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    text = update.message.text

    if text and not text.startswith('/'):
        # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–µ, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –ø–æ–∏—Å–∫
        if len(text.split()) >= 3:
            await update.message.reply_text(
                f"üí° –•–æ—Ç–∏—Ç–µ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é?\n\n"
                f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: `/search {text[:50]}`",
                parse_mode='Markdown'
            )
        elif text.lower() in ['–ø—Ä–∏–≤–µ—Ç', 'hello', 'hi', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ']:
            await update.message.reply_text("üëã –ü—Ä–∏–≤–µ—Ç! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /help –¥–ª—è —Å–ø–∏—Å–∫–∞ –∫–æ–º–∞–Ω–¥")
        elif '?' in text:
            await update.message.reply_text(
                f"‚ùì –≠—Ç–æ –≤–æ–ø—Ä–æ—Å? –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
                f"`/search {text}`",
                parse_mode='Markdown'
            )


def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    # ‚ö†Ô∏è –ó–∞–º–µ–Ω–∏—Ç–µ —Ç–æ–∫–µ–Ω –Ω–∞ –≤–∞—à!
    TOKEN = "–í–ê–®_–¢–û–ö–ï–ù_–ë–û–¢–ê"

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (–µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç)
    try:
        import bs4
    except ImportError:
        print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ BeautifulSoup4: pip install beautifulsoup4")
        return

    app = ApplicationBuilder().token(TOKEN).build()

    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("search", search))
    app.add_handler(CommandHandler("news", news))
    app.add_handler(CommandHandler("weather", weather))
    app.add_handler(CommandHandler("help", help_command))

    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ–±—ã—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    from telegram.ext import MessageHandler, filters
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, echo))

    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    print("‚úÖ –ö–æ–º–∞–Ω–¥—ã:")
    print("  /search - –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
    print("  /news - –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
    print("  /weather - –ø–æ–≥–æ–¥–∞")
    print("  /help - —Å–ø—Ä–∞–≤–∫–∞")

    app.run_polling()


if __name__ == '__main__':
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    import sys

    required_packages = ['bs4', 'aiohttp']
    missing_packages = []

    for package in required_packages:
        try:
            __import__(package.replace('-', '_'))
        except ImportError:
            missing_packages.append(package)

    if missing_packages:
        print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–∞–∫–µ—Ç—ã:")
        print(f"   pip install {' '.join(missing_packages)}")
        sys.exit(1)

    main()